---
title: "Project 2: projectpackage Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{projectpackage Tutorial}

  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
projectpackage is an R package for cross validation classification, t- test and testing hypotheses.

```{r setup}

#instructions for how to install your package from Github
library(projectpackage)
```


```{r}


```



```{r}
#A tutorial for my_t.test
#hypothesis: mean of the lifeExp data from my_gapminder equals to 60
#alternative hypothesis: mean of the lifeExp data from my_gapminder does not equal to 60
my_t_test(x=(my_gapminder$lifeExp),alternative="two sided",mu = 60)


#hypothesis: mean of the lifeExp data from my_gapminder equals to 60
#alternative hypothesis: mean of the lifeExp data from my_gapminder is smaller than 60
my_t_test(x=(my_gapminder$lifeExp),alternative="less",mu = 60)

#hypothesis: mean of the lifeExp data from my_gapminder equals to 60
#alternative hypothesis: mean of the lifeExp data from my_gapminder is larger than 60
my_t_test(x=(my_gapminder$lifeExp),alternative="greater",mu = 60)

```
For the first t-test, we observe P = 0.09322877. Therefore we conclude that we have sufficient evidence to reject the null hypothesis and conclude that the true mean life expectancy is not equal to 60.

For the second t-test, we observe P = 0.9533856 Therefore we conclude that we do not have sufficient evidence to reject the null hypothesis. Therefore, we cannot conclude that the true mean life expectancy is smaller than 60.

For the third t-test, we observe P = 0.04661438. Therefore we conclude that we  have sufficient evidence to reject the null hypothesis and conclude that the true mean life expectancy is greater than 60.
```{r}
#A tutorial for my_lm
#Demonstrate a regression using lifeExp as your response variable and gdpPercap and continent as explanatory variables
lm_model <- my_lm(lifeExp ~gdpPercap+continent, data = my_gapminder)

#Write the hypothesis test associated with the gdpPercap coefficient.
my_t_test(x= my_gapminder$gdpPercap, alternative="less", mu = 4.452704e-04)

#predict the fitted values 
my_estimate <- lm_model$Estimate
my_matrix <- model.matrix(lifeExp ~ gdpPercap + continent, data = my_gapminder)
y_hat <- my_matrix %*% as.matrix(my_estimate)
my_df <- data.frame("actual" = my_gapminder$lifeExp, "fitted" = y_hat, "continent" = my_gapminder$continent)
#Set training, testing error as a list and prediction of y for testing data
#plot the Actual vs. Fitted values.
library(ggplot2)
x <- my_gapminder$gdpPercap
y <- my_gapminder$lifeExp
fitting_plot <- ggplot(my_df, aes(x=x, y=y)) +
        geom_point(aes(x = x, y = y, col="Actual")) +
        geom_point(aes(x = x, y = my_df$fitted, col="Fitted value")) +
        # labs(subtitle = paste("Testing Error:", round(test_err_k, 3))) +
        # geom_line(aes(y = fitted(lm_fit_k[[10]])), col = "red", lwd = 1.5) +
        theme_bw()
fitting_plot
```
The Estimate contains two rows: the intercept and the slope. The intercept represents the expected value of life Expectancy given the average of gdpPercap and continent of all data. In average, it takes an average datapoint in my_gapminder 1 unit of change in gdpPercap result in a change of 47.88 in life Expectancy.

The slope of 4.45*10^-4 means that for every increase in gdpPercap, the Estimate increase by 4.452704e-04.

The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. Here, it is saying that the life Expectancy can vary by 2.349795e^-5 year. 

The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0. here, t-value of 18.94 means that there is a relationship between the response variable life Expectancy, and the explanatory variables gdpPercap and continent.

The Pr(>t) acronym found in the model output relates to the probability of observing any value equal or larger than t. A small p-value indicates that it is unlikely we will observe a relationship between the explanatory variables gdpPercap and continen, and response variables life Expectancy due to chance. here, p-value equals to 8.55e^-73, which means that there are 8.55e^-71 percent chance of observing this relationship by chance.


```{r}
#A tutorial for my_knn_cv using my_penguins.
my_table <- data.frame()
for(i in 1:10) {
  my_penguins <- drop_na(my_penguins)
  my_result <- my_knn_cv(train = my_penguins, cl = my_penguins$species, k_nn = i, k_cv = 5 )
  my_table[1,i] <- my_result$cv_err
  my_table[2,i] <- sum(as.numeric(my_penguins$species != my_result$class)) / nrow(my_penguins)
}
rownames(my_table) <- c("cv_err", "training_err")
colnames(my_table) <- c("k_nn = 1", "k_nn = 2", "k_nn = 3", "k_nn = 4", "k_nn = 5", "k_nn = 6", "k_nn = 7", "k_nn = 8", "k_nn = 9", "k_nn = 10")
```

According to the results in the table we can see that when k_nn = 1, we have both the smallest cv error and the training error, so I think k_nn = 1 is the best model when k_cv = 5. 

Cross validation is a statistical method that used to check how accurately our model can predict the result. This method is very useful since it test the model multiple time by using different parts of the same dataset, which is vert efficient.

```{r}
#A tutorial for my_rf_cv
my_table_1 <- data.frame()

for(i in 1:30) {
  my_penguins <- drop_na(my_penguins)
  result <- my_rf_cv(k = 2)
  my_table_1[1,i] <- unlist(result)
  my_table_1[2,i] <- unlist(result) / nrow(my_penguins)
}
rownames(my_table_1) <- c("MSE", "cv_err")
colnames(my_table_1) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")

my_table_2 <- data.frame()
for(i in 1:30) {
  result <- my_rf_cv(k = 5)
  my_table_2[1,i] <- unlist(result)
  my_table_2[2,i] <- unlist(result) / nrow(my_penguins)
}
rownames(my_table_2) <- c("MSE", "cv_err")
colnames(my_table_2) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")

my_table_3 <- data.frame()
for(i in 1:30) {
  result <- my_rf_cv(k = 10)
  my_table_3[1,i] <- unlist(result)
  my_table_3[2,i] <- unlist(result) / nrow(my_penguins)
}
rownames(my_table_3) <- c("MSE", "cv_err")
colnames(my_table_3) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")

```

```{r}
x <- as.numeric(my_table_1$MSE)
y <- as.numeric(my_table_1$cv_err)
ggplot(my_table_1, aes(x=x, y=y)) + geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4)
```

```{r}
average_cverr_2 <- mean(as.numeric(my_table_1[2,]))
sd_cverr_2 <- sd(as.numeric(my_table_1[2,]))
average_cverr_5 <- mean(as.numeric(my_table_2[2,]))
sd_cverr_5 <- sd(as.numeric(my_table_2[2,]))
average_cverr_10 <- mean(as.numeric(my_table_3[2,]))
sd_cverr_10 <- sd(as.numeric(my_table_3[2,]))
row_1 <- cbind(average_cverr_2, sd_cverr_2)
row_2 <- cbind(average_cverr_5, sd_cverr_5)
row_3 <- cbind(average_cverr_10, sd_cverr_10)
cv_table <- rbind(row_1, row_2, row_3)
rownames(cv_table) <- c("k = 2", "k = 5", "k = 10")
colnames(cv_table) <- c("average", "sd")
cv_table
```
We can see from the table, the average and the standard deviation of ev_err become smaller as the value of k gets larger. This is because as the k gets larger, we test our model for more times, so that it becomes more accurate. Thus the values of cv_err get smaller.
